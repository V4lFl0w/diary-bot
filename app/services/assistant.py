from __future__ import annotations

# ruff: noqa: E402
# pyright: reportOptionalSubscript=false

import os as _os
import time as _time
import contextvars as _contextvars
import uuid as _uuid


async def _send_dbg(logger, kind: str, fn, *args, **kwargs):
    """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π: –ª–æ–≥–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã/markup –∏ —Ç–µ–∫—Å—Ç (–∫–æ—Ä–æ—Ç–∫–æ)."""
    if _TRACE_ON:
        txt = None
        try:
            if "text" in kwargs and isinstance(kwargs.get("text"), str):
                txt = kwargs.get("text")[:180]
        except Exception:
            pass
        _atrace(
            logger, f"tg.{kind}.send", has_markup=bool(kwargs.get("reply_markup") or kwargs.get("markup")), text=txt
        )
    return await fn(*args, **kwargs)


_TRACE_ON = _os.getenv("TRACE_ASSISTANT", "0") == "1"
_trace_id_var: _contextvars.ContextVar[str] = _contextvars.ContextVar("atrace_id", default="")


def _atrace_id() -> str:
    return _trace_id_var.get() or "-"


def _atrace_new(prefix: str = "a") -> str:
    return f"{prefix}{_uuid.uuid4().hex[:10]}"


def _atrace(logger, stage: str, **kv):
    if not _TRACE_ON:
        return
    try:
        logger.info("[trace] %s | %s | %s", _atrace_id(), stage, kv)
    except Exception:
        pass


class _ASpan:
    def __init__(self, logger, stage: str, **kv):
        self.logger = logger
        self.stage = stage
        self.kv = kv
        self.t0 = None

    def __enter__(self):
        self.t0 = _time.time()
        _atrace(self.logger, self.stage + ".in", **self.kv)
        return self

    def __exit__(self, exc_type, exc, tb):
        dt = int((_time.time() - (self.t0 or _time.time())) * 1000)
        if exc is not None:
            _atrace(self.logger, self.stage + ".err", ms=dt, err=str(exc))
            return False
        _atrace(self.logger, self.stage + ".out", ms=dt)
        return False


def _atrace_set(tid: str):
    try:
        _trace_id_var.set(tid)
    except Exception:
        pass


def _dbg_media(logger, tag: str, **kv):
    try:
        logger.info("[media][dbg] %s | %s", tag, kv)
    except Exception:
        pass


# app/services/assistant.py
import os
import re

# --- FlowPatch: media query clean + refinement detection (assistant) ---
_TMDB_STOPWORDS = {
    "photo","<photo>","—É—Ç–æ—á–Ω–µ–Ω–∏–µ","—É—Ç–æ—á–Ω–µ–Ω–∏–µ:","—É—Ç–æ—á–Ω–∏","–¥–∞–π","–¥—Ä—É–≥–∏–µ","–≤–∞—Ä–∏–∞–Ω—Ç—ã",
    "–∂–∞–Ω—Ä","—Å—Ç—Ä–∞–Ω–∞","–≥–æ–¥","—Å–µ—Ä–∏—è","—ç–ø–∏–∑–æ–¥","—Å–µ–∑–æ–Ω",
    "film","movie","series","tv","what","is","the","a","an",
    "drama","romance","prison","fence",
}

def _tmdb_clean_user_text(text: str) -> str:
    if not text:
        return ""
    t = text.strip()
    t = t.replace("<photo>", " ").replace("photo", " ")
    t = re.sub(r"(?i)\b—É—Ç–æ—á–Ω–µ–Ω–∏–µ\s*:\s*", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    # TMDb –Ω–µ –ª—é–±–∏—Ç –ø—Ä–æ—Å—Ç—ã–Ω–∏
    if len(t) > 140:
        t = t[:140].rsplit(" ", 1)[0].strip()
    return t

def _tmdb_is_refinement(text: str) -> bool:
    if not text:
        return False
    t = text.lower().strip()

    # —è–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã "—É—Ç–æ—á–Ω—è—é/–¥–∞–π –¥—Ä—É–≥–∏–µ"
    if any(k in t for k in ("—É—Ç–æ—á–Ω–µ–Ω–∏–µ", "—É—Ç–æ—á–Ω–∏", "–¥–∞–π –¥—Ä—É–≥–∏–µ", "–¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã", "–∫–æ—Ä–æ—Ç–∫–æ")):
        return True

    # –≥–æ–¥
    if re.search(r"\b(19\d{2}|20\d{2})\b", t):
        return True

    # 1‚Äì2 —Å–ª–æ–≤–∞ –±–µ–∑ –±–æ–ª—å—à–∏—Ö –±—É–∫–≤ ‚Äî —á–∞—â–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ, –∞ –Ω–µ –Ω–æ–≤—ã–π —Ç–∞–π—Ç–ª
    parts = t.split()
    if 1 <= len(parts) <= 2 and len(t) <= 18:
        return True

    hint_words = (
        "–≥–æ–¥","–∞–∫—Ç","–∞–∫—Ç–µ—Ä","–∞–∫—Ç—ë—Ä","—Å—Ç—Ä–∞–Ω–∞","—è–∑—ã–∫","—Å–µ—Ä–∏—è","—ç–ø–∏–∑–æ–¥","—Å–µ–∑–æ–Ω",
        "—Å—à–∞","–∞–º–µ—Ä–∏–∫–∞","usa","us","uk","–Ω–µ—Ç—Ñ–ª–∏–∫—Å","netflix","hbo","amazon",
        "–∫–æ–º–µ–¥–∏—è","–¥—Ä–∞–º–∞","–±–æ–µ–≤–∏–∫","—Ç—Ä–∏–ª–ª–µ—Ä","—É–∂–∞—Å—ã","–º–µ–ª–æ–¥—Ä–∞–º–∞",
    )
    return any(w in t for w in hint_words)

def _tmdb_is_worthy_cand(q: str) -> bool:
    if not q:
        return False
    qn = q.lower().strip()
    if len(qn) < 3:
        return False
    # –æ–¥–Ω–æ —Å–ª–æ–≤–æ-—Å—Ç–æ–ø
    if " " not in qn and qn in _TMDB_STOPWORDS:
        return False
    toks = [t for t in re.split(r"[\s,.;:!?()\[\]{}\"'¬´¬ª]+", qn) if t]
    if toks and sum(1 for t in toks if t in _TMDB_STOPWORDS) / max(1, len(toks)) > 0.6:
        return False
    return True
# --- /FlowPatch ---

from datetime import datetime, timedelta, timezone
from typing import Any, Optional, cast
from zoneinfo import ZoneInfo

from sqlalchemy import desc, select

from app.models.journal import JournalEntry
from app.models.user import User
from app.services.intent_router import Intent, detect_intent
from app.services.media.formatting import (
    MEDIA_NOT_FOUND_REPLY_RU,
    MEDIA_VIDEO_STUB_REPLY_RU,
    _format_media_pick,
    _format_media_ranked,
    build_media_context,
)
from app.services.media.lens import (
    _lens_bad_candidate,
    _pick_best_lens_candidates,
)

# --- Optional OpenAI import (server may not have it) ---
# --- Anti-hallucination prefix (local-only; do not import) ---
# --- media helpers split (auto) ---
from app.services.media.logging import _d
from app.services.media.pipeline_tmdb import _tmdb_best_effort
from app.services.media.query import (
_clean_media_search_query,
    _clean_tmdb_query,
    _extract_media_kind_marker,
    _good_tmdb_cand,
    _is_asking_for_title,
    _is_bad_media_query,
    _looks_like_freeform_media_query,
    _normalize_tmdb_query,
    _parse_media_hints,
    _tmdb_sanitize_query,
    _looks_like_choice,
    _looks_like_year_or_hint,
    is_bad_tmdb_query,
    tmdb_query_compact,
    _is_bad_tmdb_candidate,
    _mf_is_worthy_tmdb,
)
from app.services.media.safety import (
    _scrub_media_items,
)
from app.services.media.session import (
_MEDIA_SESSIONS,
    _media_get,
    _media_set,
    _media_uid,
)

from app.services.media.vision_parse import (
    _build_tmdb_queries_from_media_json,
    _extract_media_json_from_model_text,
    _extract_search_query_from_text,
    _extract_title_like_from_model_text,
)

# --- compat: generic media caption detector (legacy import path) ---
try:
    from app.services.media_text import (
        is_generic_media_caption as _is_generic_media_caption,
    )  # type: ignore
except Exception:  # pragma: no cover

    def _is_generic_media_caption(text: str) -> bool:  # type: ignore
        t = (text or "").strip().lower()
        if not t:
            return True
        t = re.sub(r"\s+", " ", t).strip()
        return t in {
            "–æ—Ç–∫—É–¥–∞ –∫–∞–¥—Ä",
            "–æ—Ç–∫—É–¥–∞ –∫–∞–¥—Ä?",
            "—á—Ç–æ –∑–∞ —Ñ–∏–ª—å–º",
            "—á—Ç–æ –∑–∞ —Ñ–∏–ª—å–º?",
            "—á—Ç–æ –∑–∞ —Å–µ—Ä–∏–∞–ª",
            "—á—Ç–æ –∑–∞ —Å–µ—Ä–∏–∞–ª?",
            "—á—Ç–æ –∑–∞ –º—É–ª—å—Ç–∏–∫",
            "—á—Ç–æ –∑–∞ –º—É–ª—å—Ç–∏–∫?",
            "–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è",
            "–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è?",
        }


ANTI_HALLUCINATION_PREFIX: str = ""

try:
    from openai import AsyncOpenAI
except ModuleNotFoundError:
    AsyncOpenAI = None  # type: ignore

# --- Models (imported at top) ---

# --- Project-level constants (fallbacks) ---
# Used by _is_generic_media_caption
# _GENERIC_MEDIA_CAPTIONS moved to app/services/media/query.py
# --- restored media helpers (from assistant.py.bak2) ---


# --- restored helpers (from assistant.py.bak2) ---


# --- vision cache (screenshot -> result) ---
_VISION_IMG_CACHE: dict[str, tuple[float, str]] = {}
_VISION_IMG_CACHE_TTL_SEC = 30 * 60  # 30 minutes


def _vision_cache_get(key: str) -> str | None:
    try:
        v = _VISION_IMG_CACHE.get(key)
        if not v:
            return None
        ts, reply = v
        if (_time.time() - ts) > _VISION_IMG_CACHE_TTL_SEC:
            _VISION_IMG_CACHE.pop(key, None)
            return None
        return reply
    except Exception:
        return None


def _vision_cache_set(key: str, reply: str) -> None:
    try:
        if key and reply:
            _VISION_IMG_CACHE[key] = (_time.time(), reply)
    except Exception:
        pass


# --- safety: scrub explicit overviews (TMDb sometimes returns NSFW text even with include_adult=false) ---


# --- Services imports (try real, otherwise safe stubs) ---
try:
    from app.services.media_search import tmdb_search_multi  # expected existing
except Exception:  # pragma: no cover

    async def tmdb_search_multi(*args: Any, **kwargs: Any) -> list[dict]:
        return []


try:
    from app.services.media_web_pipeline import (
        web_to_tmdb_candidates,  # expected existing
    )
except Exception:  # pragma: no cover

    async def web_to_tmdb_candidates(*args: Any, **kwargs: Any) -> tuple[list[str], str]:
        return ([], "web_stub")


try:
    from app.services.media_web_pipeline import (
        image_bytes_to_tmdb_candidates,
    )  # expected existing
except Exception:  # pragma: no cover

    async def image_bytes_to_tmdb_candidates(*args: Any, **kwargs: Any) -> tuple[list[str], str]:
        return ([], "lens_stub")


try:
    from app.services.media_id import trace_moe_identify  # expected existing
except Exception:  # pragma: no cover

    async def trace_moe_identify(*args: Any, **kwargs: Any) -> Optional[dict]:
        return None


try:
    from app.services.llm_usage import log_llm_usage  # expected existing
except Exception:  # pragma: no cover

    async def log_llm_usage(*args: Any, **kwargs: Any) -> None:
        return None


# --- Optional project prompts (safe fallback for workers/tests) ---

# --- TMDB query sanitizer: TMDB hates long "scene description" queries ---


def _media_confident(item: dict) -> bool:
    """Conservative confidence heuristic for Vision results."""
    try:
        pop = float(item.get("popularity") or 0)
        va = float(item.get("vote_average") or 0)
    except Exception:
        return False
    return (pop >= 25 and va >= 6.8) or (pop >= 60) or (va >= 7.6)


# --- BAD OCR / GENERIC QUERY FILTER FOR MEDIA SEARCH ---

# --- media query cleaning: turn human phrasing into search-friendly query ---

# --- media session cache (in-memory, no DB migrations) ---


def _env(name: str, default: str = "") -> str:
    v = os.getenv(name)
    return v if v else default


def _pick_model() -> str:
    return _env("ASSISTANT_MODEL", "gpt-4.1-mini")


def _user_name(user: Optional[User]) -> str:
    for attr in ("first_name", "name", "username"):
        v = getattr(user, attr, None)
        if v:
            return str(v)
    return "–¥—Ä—É–≥"


def _user_tz(user: Optional[User]) -> ZoneInfo:
    tz_name = getattr(user, "tz", None) or "UTC"
    try:
        return ZoneInfo(str(tz_name))
    except Exception:
        return ZoneInfo("UTC")


def _assistant_plan(user: Optional[User]) -> str:
    if not user:
        return "free"

    now = datetime.now(timezone.utc)

    # –µ—Å–ª–∏ premium_until –µ—Å—Ç—å –∏ –æ–Ω –∏—Å—Ç—ë–∫ ‚Üí FREE
    pu = getattr(user, "premium_until", None)
    if pu is not None:
        if pu.tzinfo is None:
            pu = pu.replace(tzinfo=timezone.utc)
        if pu <= now:
            return "free"

    # –µ—Å–ª–∏ premium_until –Ω–µ—Ç –∏ is_premium=False ‚Üí FREE
    if pu is None and not bool(getattr(user, "is_premium", False)):
        return "free"

    # –ø—Ä–µ–º–∏—É–º –µ—Å—Ç—å ‚Üí —á–∏—Ç–∞–µ–º —Ç–∞—Ä–∏—Ñ
    plan = str(getattr(user, "premium_plan", "") or "").strip().lower()
    if plan in {"basic", "pro"}:
        return plan

    # –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ç–∞—Ä–∏—Ñ –ø—Ä–µ–º–∏—É–º–∞
    return "basic"


def _now_str_user(user: Optional[User]) -> str:
    tz = _user_tz(user)
    return datetime.now(tz).strftime("%Y-%m-%d %H:%M")


def _is_media_query(text: str) -> bool:
    t = (text or "").lower()
    # –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ + —Ç–∏–ø–∏—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –ø–æ–∏—Å–∫ –Ω–∞–∑–≤–∞–Ω–∏—è
    keys = (
        "—Ñ–∏–ª—å–º",
        "—Å–µ—Ä–∏–∞–ª",
        "–∫–∏–Ω–æ",
        "–º—É–ª—å—Ç",
        "–º—É–ª—å—Ç–∏–∫",
        "–ª–µ–Ω—Ç–∞",
        "–∫–∞–¥—Ä",
        "–ø–æ –∫–∞–¥—Ä—É",
        "–ø–æ —ç—Ç–æ–º—É –∫–∞–¥—Ä—É",
        "season",
        "episode",
        "movie",
        "tv",
        "series",
        "–∞–∫—Ç—ë—Ä",
        "–∞–∫—Ç–µ—Ä",
        "—Ä–µ–∂–∏—Å—Å",
        "–ø–µ—Ä—Å–æ–Ω–∞–∂",
        "–∫–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è",
        "—á—Ç–æ –∑–∞ —Ñ–∏–ª—å–º",
        "—á—Ç–æ –∑–∞ —Å–µ—Ä–∏–∞–ª",
        "—á—Ç–æ –∑–∞ –º—É–ª—å—Ç–∏–∫",
    )
    return any(k in t for k in keys)


def _is_noise(text: str) -> bool:
    s = (text or "").strip()
    if not s:
        return True
    letters = sum(ch.isalpha() for ch in s)
    if letters == 0:
        return True

    # —Å—É–ø–µ—Ä–∫–æ—Ä–æ—Ç–∫–æ–µ –ø–æ—á—Ç–∏ –≤—Å–µ–≥–¥–∞ –º—É—Å–æ—Ä (–Ω–æ 1-2 —Å–ª–æ–≤–∞ –∏–Ω–æ–≥–¥–∞ –≤–∞–∂–Ω—ã)
    if len(s) <= 3:
        return True

    tokens = re.findall(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë–Ü—ñ–á—ó–Ñ—î]+", s.lower())
    if tokens:
        most = max(tokens.count(x) for x in set(tokens))
        if most / max(1, len(tokens)) >= 0.6 and len(tokens) >= 4:
            return True

        if len(tokens) >= 4:
            uniq = set(tokens)
            if len(uniq) <= 2 and all(tokens.count(t) >= 2 for t in uniq):
                return True

    # –Ω–∏–∫/–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –±–µ–∑ –ø—Ä–æ–±–µ–ª–æ–≤ —Å –ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ–º (Pisya_Popa)
    if "_" in s and " " not in s and len(s) <= 20:
        return True

    return False


def _as_user_ts(user: Optional[User], ts: Any) -> str:
    """
    created_at –∏–∑ sqlite –º–æ–∂–µ—Ç –±—ã—Ç—å naive.
    –°—á–∏—Ç–∞–µ–º naive –∫–∞–∫ UTC (—ç—Ç–æ —Å–∞–º—ã–π –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –¥–µ—Ñ–æ–ª—Ç –¥–ª—è —Å–µ—Ä–≤–µ—Ä–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏),
    –ø–æ—Ç–æ–º –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ tz —é–∑–µ—Ä–∞.
    """
    if ts is None:
        return "?"
    try:
        tz = _user_tz(user)
        if getattr(ts, "tzinfo", None) is None:
            ts = ts.replace(tzinfo=timezone.utc)
        return ts.astimezone(tz).strftime("%Y-%m-%d %H:%M")
    except Exception:
        try:
            return ts.strftime("%Y-%m-%d %H:%M")
        except Exception:
            return "?"


async def _fetch_recent_journal(
    session: Any,
    user: Optional[User],
    *,
    limit: int = 30,
    take: int = 5,
) -> list[tuple[str, str]]:
    if not session or not user:
        return []

    q = (
        select(JournalEntry.created_at, JournalEntry.text)
        .where(JournalEntry.user_id == user.id)
        .order_by(desc(JournalEntry.created_at))
        .limit(limit)
    )
    res = await session.execute(q)
    rows = res.all()

    out: list[tuple[str, str]] = []

    for created_at, text in rows:
        txt = (text or "").strip()
        if _is_noise(txt):
            continue

        created_str = _as_user_ts(user, created_at)
        out.append((created_str, txt[:700]))
        if len(out) >= take:
            break

    return out


async def build_context(session: Any, user: Optional[User], lang: str, plan: str) -> str:
    parts: list[str] = []
    parts.append(f"Time now: {_now_str_user(user)}")

    if user:
        parts.append(
            "User: "
            f"id={getattr(user, 'id', None)}, "
            f"tg_id={getattr(user, 'tg_id', None)}, "
            f"name={_user_name(user)}, "
            f"tz={getattr(user, 'tz', None)}"
        )

        last_used = getattr(user, "assistant_last_used_at", None)
        if last_used:
            parts.append(f"Assistant last used at: {last_used}")

        profile = getattr(user, "assistant_profile_json", None)
        if profile:
            parts.append("Assistant profile (long-term):")
            parts.append(str(profile)[:2000])

    take = 0 if plan in {"free", "basic"} else 5

    recent = await _fetch_recent_journal(session, user, limit=30, take=take)
    if recent:
        parts.append("Recent journal entries:")
        for ts, txt in recent:
            parts.append(f"- [{ts}] {txt}")

    return "\n".join(parts)


def _instructions(lang: str, plan: str) -> str:
    base_map = {
        "ru": (
            "–¢—ã ‚Äî –ª–∏—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –≤ Telegram. –ü–∏—à–∏ –ø–æ-—Ä—É—Å—Å–∫–∏.\n"
            "–ù–µ –æ—Ü–µ–Ω–∏–≤–∞–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –Ω–µ –¥–µ–ª–∞–π –ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑.\n"
            "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç ‚Äî –∑–∞–¥–∞–π 1 —É—Ç–æ—á–Ω—è—é—â–∏–π –≤–æ–ø—Ä–æ—Å.\n"
        ),
        "uk": (
            "–¢–∏ ‚Äî –æ—Å–æ–±–∏—Å—Ç–∏–π –ø–æ–º—ñ—á–Ω–∏–∫ —É Telegram. –ü–∏—à–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é.\n"
            "–ù–µ –æ—Ü—ñ–Ω—é–π –Ω–∞—Å—Ç—Ä—ñ–π —ñ –Ω–µ —Ä–æ–±–∏ –ø—Å–∏—Ö–æ–∞–Ω–∞–ª—ñ–∑.\n"
            "–Ø–∫—â–æ –±—Ä–∞–∫—É—î –¥–∞–Ω–∏—Ö ‚Äî –ø–æ—Å—Ç–∞–≤ 1 —É—Ç–æ—á–Ω—é–≤–∞–ª—å–Ω–µ –ø–∏—Ç–∞–Ω–Ω—è.\n"
        ),
        "en": (
            "You are a personal Telegram assistant. Reply in English.\n"
            "Do not psychoanalyze mood.\n"
            "If info is missing ‚Äî ask 1 clarifying question.\n"
        ),
    }

    base = base_map.get(lang, base_map["en"])

    style = (
        "–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:\n"
        "- –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —à–∞–±–ª–æ–Ω—ã '–°—É—Ç—å/–ü–ª–∞–Ω/–®–∞–≥–∏' –∏ –Ω—É–º–µ—Ä–∞—Ü–∏—é, –µ—Å–ª–∏ –Ω–µ –ø—Ä–æ—Å—è—Ç.\n"
        "- –ë–µ–∑ –ø—Å–∏—Ö–æ–∞–Ω–∞–ª–∏–∑–∞ –∏ –¥–∏–∞–≥–Ω–æ–∑–æ–≤.\n"
        "- –ö–æ—Ä–æ—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n"
    )

    if plan == "basic":
        return (
            base
            + style
            + (
                "–†–µ–∂–∏–º BASIC:\n"
                "- 2‚Äì6 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n"
                "- –ë–µ–∑ –ø–ª–∞–Ω–æ–≤ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –±–µ–∑ –∑–∞–ø—Ä–æ—Å–∞.\n"
                "- –ñ—É—Ä–Ω–∞–ª –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –ø–∞–º—è—Ç—å.\n"
            )
        )

    return (
        base
        + style
        + (
            "–†–µ–∂–∏–º PRO:\n"
            "- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø–∏—Å–∏ –∂—É—Ä–Ω–∞–ª–∞ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n"
            "- –ú–æ–∂–Ω–æ –ø—Ä–µ–¥–ª–∞–≥–∞—Ç—å —á–µ–∫–ª–∏—Å—Ç—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É.\n"
            "- –ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –¥–æ 2 —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤.\n"
            "- –°—Ç–∏–ª—å: —É–º–Ω—ã–π –±–ª–∏–∑–∫–∏–π –ø–æ–º–æ—â–Ω–∏–∫.\n"
        )
    )


async def run_assistant(
    user: Optional[User],
    text: str,
    lang: str,
    *,
    session: Any = None,
    has_media: bool = False,
) -> str:
    if AsyncOpenAI is None:
        return "ü§ñ –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Å–µ—Ä–≤–µ—Ä –±–µ–∑ openai).\n–ü–æ–ø—Ä–æ–±—É–π –ø–æ–∑–∂–µ –∏–ª–∏ –Ω–∞–ø–∏—à–∏ –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É."

    api_key = _env("OPENAI_API_KEY")
    if not api_key:
        return {
            "uk": "‚ùå –ù–µ –∑–∞–¥–∞–Ω–æ OPENAI_API_KEY. –î–æ–¥–∞–π –∫–ª—é—á —É .env / –∑–º—ñ–Ω–Ω—ñ —Å–µ—Ä–µ–¥–æ–≤–∏—â–∞.",
            "en": "‚ùå OPENAI_API_KEY is missing. Add it to env/.env.",
            "ru": "‚ùå –ù–µ –∑–∞–¥–∞–Ω OPENAI_API_KEY. –î–æ–±–∞–≤—å –∫–ª—é—á –≤ .env / –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è.",
        }.get(lang, "‚ùå OPENAI_API_KEY missing.")

    client = AsyncOpenAI(api_key=api_key)
    model = _pick_model()
    plan = _assistant_plan(user)

    kind_marker = _extract_media_kind_marker(text)
    if kind_marker:
        return MEDIA_VIDEO_STUB_REPLY_RU

    now = datetime.now(timezone.utc)

    # --- MEDIA state (DB + in-memory fallback) ---
    uid = _media_uid(user)
    st = _media_get(uid)  # in-memory session, survives even if session=None

    sticky_media_db = False
    if user:
        mode = getattr(user, "assistant_mode", None)
        until = getattr(user, "assistant_mode_until", None)
        if mode == "media" and until and until > now:
            sticky_media_db = True

    # --- INTENT gate (prevents media context from leaking into other topics) ---
    intent_res = detect_intent((text or "").strip() if text else None, has_media=bool(has_media))
    intent = getattr(intent_res, "intent", None) or intent_res
    is_intent_media = intent in (Intent.MEDIA_IMAGE, Intent.MEDIA_TEXT)

    # If user message is NOT media-related, we must drop sticky media (DB + memory)
    if not is_intent_media:
        if uid:
            try:
                _MEDIA_SESSIONS.pop(uid, None)
            except Exception:
                pass
        if user is not None:
            try:
                mode = getattr(user, "assistant_mode", None)
                if mode == "media":
                    setattr(user, "assistant_mode", None)
                    setattr(user, "assistant_mode_until", now - timedelta(seconds=1))
                    if session:
                        await session.commit()
            except Exception:
                pass  # IMPORTANT: media mode should trigger ONLY for media intents (or real media message)
    # st/sticky are allowed to keep follow-ups ONLY when current intent is media.
    is_media = (
        bool(has_media)
        or bool(is_intent_media)
        or (sticky_media_db and bool(is_intent_media))
        or (bool(st) and bool(is_intent_media))
    )

    if is_media:
        _d(
            "media.enter",
            is_media=is_media,
            sticky_media_db=sticky_media_db,
            has_st=bool(st),
            uid=uid,
        )  # DBG_MEDIA_RUN_ASSISTANT_V1
        raw_text = (text or "").strip()

        # —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è: –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —Ç–æ—Ç –∂–µ –∑–∞–ø—Ä–æ—Å –∏ —É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã ‚Äî –Ω–µ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º
        try:
            if st and (st.get("items") or []) and raw_text:
                raw_norm = _tmdb_sanitize_query(_normalize_tmdb_query(raw_text.strip()))
                prev_norm = _tmdb_sanitize_query(_normalize_tmdb_query((st.get("query") or "").strip()))
                if raw_norm and prev_norm and raw_norm == prev_norm:
                    opts = st.get("items") or []

                    return _format_media_ranked(
                        prev_norm, opts, year_hint=_parse_media_hints(prev_norm).get("year"), lang=lang, source="cache"
                    )
        except Exception:
            pass

        # 1) User picked an option number: "1", "2", ...
        if st and _looks_like_choice(raw_text):
            idx = int(raw_text) - 1
            opts = st.get("items") or []
            if 0 <= idx < len(opts):
                picked = opts[idx]
                return _format_media_pick(picked) + "\n\n–•–æ—á–µ—à—å ‚Äî –Ω–∞–ø–∏—à–∏ –¥—Ä—É–≥–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/–æ–ø–∏—Å–∞–Ω–∏–µ, —è –ø–æ–∏—â—É –µ—â—ë."

        # 1.5) "–ö–∞–∫ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è/–∫–∞–∫–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ" ‚Äî —ç—Ç–æ –Ω–µ –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã
        if st and _is_asking_for_title(raw_text):
            opts = st.get("items") or []
            if not opts:
                return MEDIA_NOT_FOUND_REPLY_RU
            return build_media_context(opts) + "\n\n–ö–Ω–æ–ø–∫–∏: ‚úÖ –≠—Ç–æ –æ–Ω–æ / üîÅ –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã / üß© –£—Ç–æ—á–Ω–∏—Ç—å"
        # 2) Build query (new query vs follow-up hint)# 2) Merge —É—Ç–æ—á–Ω–µ–Ω–∏–µ with previous query
        # 2) Build query (new query vs follow-up hint)
        raw = raw_text
        prev_q = ((st.get("query") if st else "") or "").strip()

        # –Ω–µ –¥–∞—ë–º "—è–¥–æ–≤–∏—Ç—ã–º" —Ñ—Ä–∞–∑–∞–º –ø–æ—Ä—Ç–∏—Ç—å –ø–æ–∏—Å–∫–æ–≤—É—é —Å—Ç—Ä–æ–∫—É
        if st and re.search(
            r"(?i)\b(–Ω–µ\s*—Ç–æ|–Ω–µ\s*–ø–æ–¥—Ö–æ–¥–∏—Ç|–Ω–∏—á–µ–≥–æ\s*–Ω–µ|—Ç–∞–∫–æ–≥–æ\s*—Ñ–∏–ª—å–º–∞|–Ω–µ\s*—Å—É—â–µ—Å—Ç–≤—É–µ—Ç)\b",
            raw,
        ):
            return MEDIA_NOT_FOUND_REPLY_RU

        # –∫–æ—Ä–æ—Ç–∫–æ–µ —É—Ç–æ—á–Ω–µ–Ω–∏–µ (–≥–æ–¥/–∞–∫—Ç—ë—Ä/—Å—Ç—Ä–∞–Ω–∞/—è–∑—ã–∫/—Å–µ—Ä–∏—è/—ç–ø–∏–∑–æ–¥) ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –∫ –ø—Ä–æ—à–ª–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        raw = _normalize_tmdb_query(raw)

        # –µ—Å–ª–∏ –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–∞—è media-—Å–µ—Å—Å–∏—è –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–∏—Å–ª–∞–ª —É—Ç–æ—á–Ω–µ–Ω–∏–µ ‚Äî –ø—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –∫ –ø—Ä–æ—à–ª–æ–º—É –∑–∞–ø—Ä–æ—Å—É
        # (–Ω–µ —Ç–æ–ª—å–∫–æ –≥–æ–¥/–∞–∫—Ç—ë—Ä, –Ω–æ –∏ –∫–æ—Ä–æ—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã)
        if st and prev_q and raw and (len(raw) <= 140):
            raw_l = raw.lower().strip()
            prev_l = prev_q.lower().strip()

            def _is_strong_candidate(q: str) -> bool:
                q = (q or "").strip()
                if not q:
                    return False
                # –≥–æ–¥/–∫–æ—Ä–æ—Ç–∫–∏–π —Ö–∏–Ω—Ç
                if _looks_like_year_or_hint(q):
                    return True
                # —Ç–∞–π—Ç–ª + –≥–æ–¥ (Inception 2010)
                if re.search(r"\b(19\d{2}|20\d{2})\b", q) and len(q) <= 80:
                    return True
                # –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π tmdb-–∫–∞–Ω–¥–∏–¥–∞—Ç: –∫–æ—Ä–æ—Ç–∫–∏–π, –±–µ–∑ –º—É—Å–æ—Ä–∞
                if _good_tmdb_cand(q) and len(q) <= 80:
                    return True
                return False

            # ‚úÖ –∫–ª—é—á–µ–≤–æ–π —Ñ–∏–∫—Å: —Å–∏–ª—å–Ω—ã–π –∫–∞–Ω–¥–∏–¥–∞—Ç –ù–ï —Å–º–µ—à–∏–≤–∞–µ–º —Å –ø—Ä–æ—à–ª—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
            if _is_strong_candidate(raw):
                query = _tmdb_sanitize_query(_normalize_tmdb_query(raw))
            # –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–æ–≤—Ç–æ—Ä–∏–ª –ø—Ä–æ—à–ª—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî –Ω–µ –¥–µ—Ä–≥–∞–µ–º –Ω–æ–≤—ã–π –ø–æ–∏—Å–∫
            elif raw_l == prev_l:
                query = _tmdb_sanitize_query(_normalize_tmdb_query(prev_q))
            # –µ—Å–ª–∏ –≤ —É—Ç–æ—á–Ω–µ–Ω–∏–∏ —É–∂–µ –µ—Å—Ç—å –ø—Ä–æ—à–ª—ã–π –∑–∞–ø—Ä–æ—Å ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ –∫–∞–∫ –µ—Å—Ç—å
            elif prev_l and (prev_l in raw_l):
                query = _tmdb_sanitize_query(_normalize_tmdb_query(raw))
            else:
                query = _tmdb_sanitize_query(_normalize_tmdb_query(f"{prev_q} {raw}"))
        else:
            query = _tmdb_sanitize_query(_clean_media_search_query(raw))

        # FlowPatch: final media built_query guard (refinement-safe, no sticky garbage glue)
        try:
            raw_clean = _tmdb_clean_user_text(raw or "")
            prev_clean = _tmdb_clean_user_text(prev_q or "")
            if raw_clean:
                raw = raw_clean
            if prev_clean:
                prev_q = prev_clean
            if raw_clean and _tmdb_is_refinement(raw_clean):
                # —É—Ç–æ—á–Ω–µ–Ω–∏–µ ‚Äî –ù–ï –∫–ª–µ–∏–º –∫ prev_q
                query = _tmdb_sanitize_query(_normalize_tmdb_query(raw_clean))
            else:
                # –Ω–∞ –≤—Å—è–∫–∏–π: —á–∏—Å—Ç–∏–º query –æ—Ç —Å–ª—É–∂–µ–±–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞
                query = _tmdb_sanitize_query(_normalize_tmdb_query(_tmdb_clean_user_text(query or "")))
        except Exception:
            pass

        _d("media.built_query", prev_q=prev_q, raw=raw, query=query)


    # --- FlowPatch: stabilize media query (prevent Lovers/Chuck Keep overwrite) ---
    # –ü—Ä–∞–≤–∏–ª–æ:
    # 1) –µ—Å–ª–∏ —É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å prev_q (—Å–∏–ª—å–Ω—ã–π –ø—Ä–æ—à–ª—ã–π –∑–∞–ø—Ä–æ—Å), –ù–ï –∑–∞–º–µ–Ω—è–µ–º –µ–≥–æ —Å–ª–∞–±—ã–º –º—É—Å–æ—Ä–æ–º
    # 2) —Å–ª–∞–±—ã–º —Å—á–∏—Ç–∞–µ–º: is_bad_tmdb_query / _is_bad_tmdb_candidate / not _mf_is_worthy_tmdb
    # 3) –µ—Å–ª–∏ raw_text –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–∞–π—Ç–ª ‚Äî –ø–æ–¥–Ω–∏–º–∞–µ–º –µ–≥–æ –≤—ã—à–µ lens-—à—É–º–∞
    try:
        prev_q_n = (prev_q or "").strip()
        q_n = (query or "").strip()
        raw_n = (raw or "").strip() if "raw" in locals() else (raw_text or "").strip()

        # –∫–∞–Ω–¥–∏–¥–∞—Ç –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–∫–æ—Ä–æ—á–µ –∏ "title-ish")
        raw_titleish = tmdb_query_compact(raw_n) if raw_n else ""
        if raw_titleish and not is_bad_tmdb_query(raw_titleish):
            # –µ—Å–ª–∏ query —Å–µ–π—á–∞—Å —Å–ª–∞–±—ã–π ‚Äî –ø–æ–¥–º–µ–Ω—è–µ–º –Ω–∞ title-ish –∏–∑ —Ç–µ–∫—Å—Ç–∞
            if (not q_n) or is_bad_tmdb_query(q_n) or _is_bad_tmdb_candidate(q_n) or (not _mf_is_worthy_tmdb(q_n)):
                query = raw_titleish
                q_n = raw_titleish

        # –µ—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ —Å–ª–∞–±—ã–π, –Ω–æ –µ—Å—Ç—å prev_q ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ prev_q
        if prev_q_n and (not q_n or is_bad_tmdb_query(q_n) or _is_bad_tmdb_candidate(q_n) or (not _mf_is_worthy_tmdb(q_n))):
            query = prev_q_n
            q_n = prev_q_n

        # –¥–æ–ø. –∑–∞—â–∏—Ç–∞: "Lovers"/–æ–¥–∏–Ω–æ—á–Ω—ã–µ –æ–±—â–∏–µ —Å–ª–æ–≤–∞ –Ω–µ –¥–æ–ª–∂–Ω—ã –∑–∞–º–µ–Ω—è—Ç—å prev_q
        if prev_q_n and q_n and (" " not in q_n) and len(q_n) <= 10:
            if _is_bad_tmdb_candidate(q_n) or (not _mf_is_worthy_tmdb(q_n)):
                query = prev_q_n
    except Exception:
        pass
    # --- /FlowPatch ---

        # 3) Too generic ‚Üí ask 1 detail
        if len(query) < 6 and ("—Ñ–∏–ª—å–º" in query.lower() or "—á—Ç–æ –∑–∞" in query.lower()):
            # keep media mode alive for follow-ups even without DB session
            if user is not None:
                setattr(user, "assistant_mode", "media")
                setattr(user, "assistant_mode_until", now + timedelta(minutes=10))
                if session:
                    await session.commit()
            return MEDIA_NOT_FOUND_REPLY_RU

        # 4) Best-effort TMDb search (ru first, fallback en, year filter, dedupe, sort)
        cleaned = _normalize_tmdb_query(query)
        query = _tmdb_sanitize_query(_normalize_tmdb_query(cleaned or query))

        try:
            items = []

            # üîπ First try direct search by model/caption query
            items = await _tmdb_best_effort(query, limit=5)
            items = _scrub_media_items(items)
            _d(
                "media.tmdb.primary",
                q=query,
                n=len(items or []),
                top=((items or [{}])[0].get("title") or (items or [{}])[0].get("name")) if items else None,
            )

            # üîπ If nothing found ‚Äî use parsed hints
            hints = _parse_media_hints(query)
            if (not items) and hints.get("keywords"):
                items = await _tmdb_best_effort(hints["keywords"], limit=5)

            if not items and hints.get("cast"):
                from app.services.media_search import (
                    tmdb_discover_with_people,
                    tmdb_search_person,
                )

                for actor in hints["cast"]:
                    pid = await tmdb_search_person(actor)
                    if pid:
                        items = await tmdb_discover_with_people(
                            pid,
                            year=hints.get("year"),
                            kind=hints.get("kind"),
                        )
                        if items:
                            break

        except Exception:
            items = []

        # If user sent a long free-form scene description, TMDb guesses are often noisy.
        # In that case, force WEB pipeline to extract the real title.
        try:
            if items and raw and _looks_like_freeform_media_query(raw):
                items = []
        except Exception:
            pass

        # --- WEB fallback (cheap -> expensive) ---
        # –ø–æ—Ä—è–¥–æ–∫:
        # 1) wiki/brave (–±–µ–∑ SerpAPI)
        # 2) SerpAPI —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á
        if not items and query:
            query = _normalize_tmdb_query(query)

            async def _try_cands(cands: list[str]) -> list[dict]:
                out: list[dict] = []
                for c in (cands or [])[:15]:
                    if _is_bad_media_query(c):
                        continue
                    c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
                    if not _good_tmdb_cand(c):
                        continue
                    out = await _tmdb_best_effort(c, limit=5)
                    if out:
                        return out
                return out

            try:
                # --- media refinement guard ---
                # If user sends non-digit while media session is active, treat it as query refinement.
                if is_intent_media and (st or sticky_media_db) and text:
                    t = text.strip()
                    if t and (not re.fullmatch(r"\d+", t)) and (not t.startswith("/")):
                        query = t
                        items = []
                # --- end guard ---
                cands, tag = await web_to_tmdb_candidates(query, use_serpapi=False)
                _d(
                    "media.web.cands",
                    use_serpapi=False,
                    tag=tag,
                    n=len(cands or []),
                    sample=(cands or [])[:5],
                )
                items = await _try_cands(cands)
            except Exception:
                items = []

            # SerpAPI ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤—Å—ë –µ—â—ë –ø—É—Å—Ç–æ –∏ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –∫–ª—é—á
            if (not items) and (os.getenv("SERPAPI_API_KEY") or os.getenv("SERPAPI_KEY")):
                try:
                    cands, tag = await web_to_tmdb_candidates(query, use_serpapi=True)
                    _d(
                        "media.web.cands_serp",
                        use_serpapi=True,
                        tag=tag,
                        n=len(cands or []),
                        sample=(cands or [])[:5],
                    )
                    items = await _try_cands(cands)
                except Exception:
                    pass

        # keep sticky media mode (DB if possible)
        if user is not None:
            setattr(user, "assistant_mode", "media")
            setattr(user, "assistant_mode_until", now + timedelta(minutes=10))
            if session:
                await session.commit()

        if not items:
            # keep last query in memory so next hint still treated as media
            if uid:
                _media_set(uid, query, [])
            return MEDIA_NOT_FOUND_REPLY_RU

        items = _scrub_media_items(items)
        if uid:
            _media_set(uid, query, items)
        return _format_media_ranked(
            query, items, year_hint=_parse_media_hints(query).get("year"), lang=lang, source="tmdb"
        )

    # ---- Normal assistant (non-media) ----
    ctx = await build_context(session, user, lang, plan)

    prev_id = getattr(user, "assistant_prev_response_id", None) if user else None
    if user:
        last_used = getattr(user, "assistant_last_used_at", None)
        if last_used and (datetime.now(timezone.utc) - last_used) > timedelta(hours=24):
            prev_id = None

    prompt = f"Context:\n{ctx}\n\nUser message:\n" + (text or "") + "\n"

    resp = await client.responses.create(
        previous_response_id=prev_id,
        model=model,
        instructions=_instructions(lang, plan),
        input=prompt,
        max_output_tokens=(260 if plan == "basic" else 650),
    )

    if session:
        await log_llm_usage(
            session,
            user_id=getattr(user, "id", None) if user else None,
            feature="assistant",
            model=model,
            plan=plan,
            resp=resp,
            meta={"lang": lang},
        )

    out_text = (getattr(resp, "output_text", None) or "").strip()

    resp_id = getattr(resp, "id", None)
    if session and user and resp_id:
        changed = False
        if user.assistant_prev_response_id != str(resp_id):
            user.assistant_prev_response_id = str(resp_id)
            changed = True
        user.assistant_last_used_at = datetime.now(timezone.utc)
        changed = True

        if changed:
            await session.commit()

    if out_text:
        return out_text

    try:
        return str(getattr(resp, "output", "")).strip() or "‚ö†Ô∏è Empty response."
    except Exception:
        return "‚ö†Ô∏è –ù–µ —Å–º–æ–≥ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏."


async def run_assistant_vision(
    user: Optional[User],
    image_bytes: bytes,
    caption: str,
    lang: str,
    *,
    session: Any = None,
) -> str:
    if AsyncOpenAI is None:
        return "ü§ñ Vision –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (—Å–µ—Ä–≤–µ—Ä –±–µ–∑ openai)."

    api_key = _env("OPENAI_API_KEY")
    if not api_key:
        return {
            "uk": "‚ùå –ù–µ –∑–∞–¥–∞–Ω–æ OPENAI_API_KEY.",
            "en": "‚ùå OPENAI_API_KEY is missing.",
            "ru": "‚ùå –ù–µ –∑–∞–¥–∞–Ω OPENAI_API_KEY.",
        }.get(lang, "‚ùå OPENAI_API_KEY missing.")

    plan = _assistant_plan(user)
    if plan != "pro":
        return {
            "ru": "–§–æ—Ç–æ –¥–æ—Å—Ç—É–ø–Ω–æ —Ç–æ–ª—å–∫–æ –≤ PRO.",
            "uk": "–§–æ—Ç–æ –¥–æ—Å—Ç—É–ø–Ω–µ –ª–∏—à–µ –≤ PRO.",
            "en": "Photos are PRO-only.",
        }.get(lang, "PRO-only.")

    client = AsyncOpenAI(api_key=api_key)

    prompt_text = (caption or "").strip() or {
        "ru": "–û–ø—Ä–µ–¥–µ–ª–∏, —á—Ç–æ –Ω–∞ —Ñ–æ—Ç–æ. –ï—Å–ª–∏ —ç—Ç–æ –∫–∞–¥—Ä –∏–∑ —Ñ–∏–ª—å–º–∞/—Å–µ—Ä–∏–∞–ª–∞/–º—É–ª—å—Ç–∞ ‚Äî –ø–æ–ø—Ä–æ–±—É–π –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫.",
        "uk": "–í–∏–∑–Ω–∞—á, —â–æ –Ω–∞ —Ñ–æ—Ç–æ. –Ø–∫—â–æ —Ü–µ –∫–∞–¥—Ä –∑ —Ñ—ñ–ª—å–º—É/—Å–µ—Ä—ñ–∞–ª—É/–º—É–ª—å—Ç—Ñ—ñ–ª—å–º—É ‚Äî —Å–ø—Ä–æ–±—É–π –≤–∏–∑–Ω–∞—á–∏—Ç–∏ –¥–∂–µ—Ä–µ–ª–æ.",
        "en": "Identify what‚Äôs in the image. If it‚Äôs a movie/series/cartoon frame, try to identify the source.",
    }.get(
        lang,
        "Identify the image and, if it's a movie/series/cartoon frame, try to identify the source.",
    )

    hard_keywords = (
        "—Ç–µ–∫—Å—Ç",
        "—á—Ç–æ –Ω–∞–ø–∏—Å–∞–Ω–æ",
        "–ø—Ä–æ—á–∏—Ç–∞–π",
        "—Å–∫—Ä–∏–Ω",
        "—Å–∫—Ä–∏–Ω—à–æ—Ç",
        "–æ—à–∏–±–∫–∞",
        "error",
        "traceback",
        "–ª–æ–≥",
        "qr",
        "–∫—å—é–∞—Ä",
        "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è",
        "–º–µ–Ω—é",
        "—á–µ–∫",
        "—Ä–µ—Ü–µ–ø—Ç",
        "—Å–æ—Å—Ç–∞–≤",
    )
    is_hard = any(k in prompt_text.lower() for k in hard_keywords)

    model_default = _env("ASSISTANT_VISION_MODEL", _pick_model())
    model_hard = _env("ASSISTANT_VISION_MODEL_HARD", model_default)
    model = model_hard if is_hard else model_default

    import base64

    b64 = base64.b64encode(image_bytes).decode("utf-8")
    data_url = f"data:image/jpeg;base64,{b64}"

    now = datetime.now(timezone.utc)

    # --- cache by image bytes (avoid repeated Vision/Lens/TMDb on same screenshot) ---
    img_key = ""
    try:
        import hashlib

        img_key = hashlib.sha256(image_bytes).hexdigest()
    except Exception:
        img_key = ""
    if img_key:
        cached = _vision_cache_get(img_key)
        if cached:
            return cached

    instr = (
        ANTI_HALLUCINATION_PREFIX
        + _instructions(lang, plan)
        + "\n"
        + (
            "–¢—ã –≤–∏–¥–∏—à—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.\n"
            "–ï—Å–ª–∏ —ç—Ç–æ –∫–∞–¥—Ä –∏–∑ —Ñ–∏–ª—å–º–∞/—Å–µ—Ä–∏–∞–ª–∞/–º—É–ª—å—Ç–∞/–∞–Ω–∏–º–µ ‚Äî –ø–æ–º–æ–≥–∏ –Ω–∞–π—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫.\n"
            "–ï—Å–ª–∏ –Ω–µ —É–≤–µ—Ä–µ–Ω ‚Äî —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏. –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –¥–µ—Ç–∞–ª–∏.\n\n"
            "–í–ê–ñ–ù–û: –∏–≥–Ω–æ—Ä–∏—Ä—É–π —Ö—ç—à—Ç–µ–≥–∏ (#...), –Ω–∏–∫–Ω–µ–π–º—ã (@...), —ç–º–æ–¥–∑–∏, UI-–∫–Ω–æ–ø–∫–∏ (Subscribe/Like/Share),\n"
            "–Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞–Ω–∞–ª–æ–≤, –º—É–∑—ã–∫—É/–Ω–∞–∑–≤–∞–Ω–∏–µ —Ç—Ä–µ–∫–∞, –ª–∞–π–∫–∏/–ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∏ –¥–µ–∫–æ—Ä–∞—Ç–∏–≤–Ω—ã–π —Ç–µ–∫—Å—Ç.\n\n"
            "–°–Ω–∞—á–∞–ª–∞ –≤–µ—Ä–Ω–∏ –°–¢–†–û–ì–û JSON (–±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π):\n"
            '{"actors":["..."],"title_hints":["..."],"keywords":["..."]}\n'
            "- actors: –∏–º–µ–Ω–∞ –∞–∫—Ç—ë—Ä–æ–≤/–∞–∫—Ç—Ä–∏—Å (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ –ª–∞—Ç–∏–Ω–∏—Ü–µ–π), –º–∏–Ω–∏–º—É–º 2 –µ—Å–ª–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã\n"
            "- title_hints: –≤–æ–∑–º–æ–∂–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ/–≤–∏–¥–∏–º—ã–π —Ç–∞–π—Ç–ª (–µ—Å–ª–∏ –µ—Å—Ç—å)\n"
            "- keywords: 2‚Äì5 –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ –ø—Ä–æ —Å—Ü–µ–Ω—É/–∂–∞–Ω—Ä (EN –∏–ª–∏ RU)\n\n"
            "–ü–û–¢–û–ú (–Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ) –¥–æ–±–∞–≤—å:\n"
            "SEARCH_QUERY: <–∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å, –º–∞–∫—Å–∏–º—É–º 6‚Äì8 —Å–ª–æ–≤>\n"
        )
    )

    try:
        resp = await client.responses.create(
            model=model,
            instructions=instr,
            input=cast(
                Any,
                [
                    {
                        "role": "user",
                        "content": [
                            {"type": "input_text", "text": prompt_text},
                            {"type": "input_image", "image_url": data_url},
                        ],
                    }
                ],
            ),
            max_output_tokens=450,
        )
    except Exception as e:
        return {
            "ru": f"‚ö†Ô∏è –ù–µ —Å–º–æ–≥ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–æ—Ç–æ ({type(e).__name__}). –ü–æ–ø—Ä–æ–±—É–π –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Ñ–æ—Ç–æ –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –∏–ª–∏ —Å–∂–∞—Ç—å —Å–∫—Ä–∏–Ω.",
            "uk": f"‚ö†Ô∏è –ù–µ –∑–º—ñ–≥ –æ–±—Ä–æ–±–∏—Ç–∏ —Ñ–æ—Ç–æ ({type(e).__name__}). –°–ø—Ä–æ–±—É–π –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –º–µ–Ω—à–µ —Ñ–æ—Ç–æ –∞–±–æ —Å—Ç–∏—Å–Ω—É—Ç–∏ —Å–∫—Ä—ñ–Ω.",
            "en": f"‚ö†Ô∏è I couldn‚Äôt process the photo ({type(e).__name__}). Try sending a smaller image or compress the screenshot.",
        }.get(lang, f"‚ö†Ô∏è Vision error: {type(e).__name__}")

    if session:
        await log_llm_usage(
            session,
            user_id=getattr(user, "id", None) if user else None,
            feature="vision",
            model=model,
            plan=plan,
            resp=resp,
            meta={"lang": lang},
        )

    out_text = (getattr(resp, "output_text", None) or "").strip()
    out_text = str(out_text)

    def _norm_lens_candidate(x: str) -> str:
        try:
            x = (x or "").strip()
            if not x:
                return ""
            # drop common junk tokens
            x = re.sub(
                r"\b(1080p|720p|2160p|4k|hdr|webrip|brrip|bluray|dvdrip|hdtv|x264|x265|hevc|aac|dts)\b",
                "",
                x,
                flags=re.I,
            )
            x = re.sub(r"\b(season\s*\d+|s\d{1,2}e\d{1,2}|episode\s*\d+)\b", "", x, flags=re.I)
            x = re.sub(r"[\[\]\(\)\{\}]", " ", x)
            x = re.sub(r"\s{2,}", " ", x).strip(" -:;,.\t\n\r")
            # hard cap length
            return x[:120]
        except Exception:
            return (x or "").strip()[:120]

    # trace.moe (anime) ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —è–≤–Ω–æ —Å–∫–∞–∑–∞–ª–∞ "–∞–Ω–∏–º–µ"
    if any(k in out_text.lower() for k in ("–∞–Ω–∏–º–µ", "anime")):
        try:
            result = await trace_moe_identify(image_bytes)
        except Exception:
            result = None

        if result:
            sim = float(result.get("similarity", 0) or 0)
            if sim >= 0.9:
                return (
                    "üé¨ –≠—Ç–æ –∫–∞–¥—Ä –∏–∑ –∞–Ω–∏–º–µ.\n\n"
                    f"–ù–∞–∑–≤–∞–Ω–∏–µ: {result.get('title')}\n"
                    f"–°–µ—Ä–∏—è: {result.get('episode')}\n"
                    f"–°–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {sim:.1%}"
                )
    # –∏–Ω–∞—á–µ ‚Äî –Ω–µ –ª–æ–º–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫, –ø—Ä–æ—Å—Ç–æ –∏–¥—ë–º –¥–∞–ª—å—à–µ (TMDb)

    # --- Lens (bytes -> Spaces -> Google Lens -> candidates -> TMDb) ---
    # NOTE: works only if image_bytes is a real JPG/PNG; dummy bytes will produce empty cands.
    try:
        lens_cands, lens_tag = await image_bytes_to_tmdb_candidates(
            image_bytes,
            ext="jpg",
            use_serpapi_lens=True,
            hl=("ru" if (lang or "ru") == "ru" else "en"),
            prefix="frames",
        )
    except Exception:
        lens_cands, lens_tag = [], "lens_fail"
    _d("vision.lens", lens_tag=lens_tag, lens_cands=(lens_cands or [])[:8])  # DBG_VISION_LENS_V2
    best_lens_fallback: list[str] = []

    if lens_cands:
        try:
            items = []
            used_cand = ""

            ordered = _pick_best_lens_candidates(lens_cands, limit=12)
            ordered = (ordered or [])[:5]  # hard cap: 3‚Äì5 clean candidates

            best_lens_fallback = ordered[:8]
            _d("vision.lens.pick", ordered=ordered[:10])

            for cand in ordered:
                cand0 = _norm_lens_candidate(cand)
                if not cand0:
                    continue
                # hard drop obvious junk BEFORE touching TMDb
                if _lens_bad_candidate(cand0):
                    continue

                cand0 = _normalize_tmdb_query(_clean_tmdb_query(cand0))
                if not cand0 or len(cand0) < 3:
                    continue
                if _is_bad_media_query(cand0):
                    continue

                cand0 = _tmdb_sanitize_query(_normalize_tmdb_query(cand0))
                if not _good_tmdb_cand(cand0):
                    continue
                items = await _tmdb_best_effort(cand0, limit=5)
                if items:
                    used_cand = cand0
                    break

        except Exception:
            items = []
            used_cand = ""

        if items:
            if user is not None:
                setattr(user, "assistant_mode", "media")
                setattr(user, "assistant_mode_until", now + timedelta(minutes=10))
                if session:
                    await session.commit()

            uid = _media_uid(user)
            if uid and used_cand:
                _media_set(uid, used_cand, items)
            if items:
                reply = _format_media_ranked(
                    used_cand, items, year_hint=_parse_media_hints(used_cand).get("year"), lang=lang, source="tmdb"
                )
                if img_key:
                    _vision_cache_set(img_key, reply)
                return reply

    # Vision ‚Üí TMDb candidates (robust)

    # Vision ‚Üí TMDb candidates (robust)
    caption_str = (caption or "").strip()

    _d(
        "vision.model_out",
        caption=caption_str[:120],
        out_text=(out_text or "")[:250],
        is_generic_caption=_is_generic_media_caption(caption_str),
    )  # DBG_VISION_MODEL_OUT_V2
    # Prefer explicit SEARCH_QUERY from model, then title extracted from the explanation.
    search_q = _normalize_tmdb_query(_extract_search_query_from_text(out_text))
    title_from_text = _normalize_tmdb_query(_extract_title_like_from_model_text(out_text))
    _d("vision.extract", search_q=search_q, title_from_text=title_from_text)  # DBG_VISION_EXTRACT_V2

    # CAND_LIST_JSON_PRIORITY_V1
    try:
        mj = _extract_media_json_from_model_text(out_text)
        json_queries = _build_tmdb_queries_from_media_json(mj)
        _d("vision.json", json_queries=(json_queries or [])[:10])
    except Exception as e:
        _d("vision.json.fail", err=type(e).__name__, msg=str(e)[:200])
        json_queries = []

    # Build candidate list in priority order (JSON -> model text)
    # Build candidate list in priority order:
    # 1) Vision JSON (actors/title_hints/keywords)
    # 2) Model SEARCH_QUERY / title extracted from text
    # 3) Caption (only if not generic)
    # 4) Lens fallback (only after Vision sources)
    cand_list: list[str] = []

    for c in json_queries or []:
        c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
        if c and _good_tmdb_cand(c) and c not in cand_list:
            cand_list.append(c)

    for c in (search_q, title_from_text):
        c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
        if c and _good_tmdb_cand(c) and c not in cand_list:
            cand_list.append(c)

    # Caption is used ONLY if it is not a generic phrase like "–û—Ç–∫—É–¥–∞ –∫–∞–¥—Ä?"
    if caption_str and (not _is_generic_media_caption(caption_str)):
        c = _tmdb_sanitize_query(_normalize_tmdb_query(caption_str))
        if c and _good_tmdb_cand(c) and c not in cand_list:
            cand_list.append(c)

    # Lens fallback goes LAST (weak source)
    for c in (best_lens_fallback or [])[:8]:
        c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
        if c and _good_tmdb_cand(c) and c not in cand_list:
            cand_list.append(c)

    # FlowPatch: filter vision cand_list (drop generic noise)
    try:
        cand_list = [c for c in cand_list if _tmdb_is_worthy_cand(_tmdb_clean_user_text(c))]
    except Exception:
        pass
    _d("vision.cand_list", cand_list=cand_list[:15])  # DBG_VISION_CAND_LIST_V3

    if not cand_list:
        return MEDIA_NOT_FOUND_REPLY_RU

    items: list[dict] = []
    used_query = ""

    # Try TMDb for candidates (parallel + scoring)
    import asyncio as _asyncio
    import re as _re

    def _norm_title(x: str) -> str:
        x = (x or "").lower().strip()
        x = _re.sub(r"[^\w\s]+", " ", x, flags=_re.U)
        x = _re.sub(r"\s+", " ", x).strip()
        return x

    def _score_item(item: dict, q: str) -> float:
        # vote_average + bonus for title similarity
        try:
            vote = float(item.get("vote_average") or 0.0)
        except Exception:
            vote = 0.0

        t = _norm_title(item.get("title") or item.get("name") or "")
        qn = _norm_title(q)
        bonus = 0.0
        if t and qn:
            # –ø—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞: –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ + –ø—Ä–µ—Ñ–∏–∫—Å
            tw = set(t.split())
            qw = set(qn.split())
            inter = len(tw & qw)
            bonus += min(2.0, inter * 0.35)
            if t.startswith(qn) or qn.startswith(t):
                bonus += 1.25
        # –ª—ë–≥–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ—Å—Ç–µ—Ä–∞ (—á–∞—â–µ –Ω–æ—Ä–º —Ç–∞–π—Ç–ª)
        if (item.get("poster_path") or "").strip():
            bonus += 0.25
        return vote + bonus

    # –æ–≥—Ä–∞–Ω–∏—á–∏–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, —á—Ç–æ–±—ã –Ω–µ —Å–ª–æ–≤–∏—Ç—å 429
    _sem = _asyncio.Semaphore(4)

    async def _tmdb_try(q: str):
        if not _good_tmdb_cand(q):
            return (q, [])
        async with _sem:
            try:
                qq = _normalize_tmdb_query(q)
                items = await _tmdb_best_effort(qq, limit=5)
                return (q, items or [])
            except Exception:
                return (q, [])

    # –≥–æ–Ω—è–µ–º –ø–∞—á–∫—É –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    _cands = list(cand_list or [])[:14]
    tasks = [_tmdb_try(q) for q in _cands]
    results = await _asyncio.gather(*tasks, return_exceptions=False) if tasks else []

    # –≤—ã–±–∏—Ä–∞–µ–º TOP-3 —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–æ–¥–Ω–∞ –∫–∞—Ä—Ç–æ—á–∫–∞, 1 —Ñ–æ—Ç–æ –º–∞–∫—Å–∏–º—É–º)
    best = None
    best_rows = []  # list[(score, q, item)]
    seen_ids = set()

    for q, items in results:
        for it in (items or [])[:8]:
            tid = it.get("id")
            if tid is None:
                continue
            if tid in seen_ids:
                continue
            seen_ids.add(tid)
            sc = _score_item(it, q)
            best_rows.append((sc, q, it))

    best_rows.sort(key=lambda x: x[0], reverse=True)
    top = best_rows[:3]

    def _short_overview(it: dict, max_len: int = 130) -> str:
        try:
            t = (it.get("overview") or "").strip()
        except Exception:
            t = ""
        if not t:
            return ""
        t = _re.sub(r"\s+", " ", t).strip()
        return (t[: max_len - 1] + "‚Ä¶") if len(t) > max_len else t

    def _tmdb_item_url(it: dict) -> str:
        mid = it.get("id")
        mt = (it.get("media_type") or "").strip() or ("tv" if it.get("name") else "movie")
        if not mid:
            return ""
        return f"https://www.themoviedb.org/{mt}/{mid}"

    def _item_title(it: dict) -> str:
        return (it.get("title") or it.get("name") or "").strip()

    def _item_year(it: dict) -> str:
        dt = (it.get("release_date") or it.get("first_air_date") or "").strip()
        return dt[:4] if len(dt) >= 4 else ""

    if top:
        # sticky media mode
        try:
            if user is not None:
                setattr(user, "assistant_mode", "media")
                setattr(user, "assistant_mode_until", now + timedelta(minutes=10))
                if session:
                    await session.commit()
        except Exception:
            pass

        used_query = top[0][1]  # query –ª—É—á—à–µ–≥–æ
        top_items = [row[2] for row in top]

        # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏–º–µ–Ω–Ω–æ –¢–û–ü-3, —á—Ç–æ–±—ã –≤—ã–±–æ—Ä 1/2/3 —Ä–∞–±–æ—Ç–∞–ª –¥–∞–ª—å—à–µ
        uid = _media_uid(user)
        if uid:
            _media_set(uid, used_query, top_items)

        TMDB_IMG = "https://image.tmdb.org/t/p/w342"
        poster_url = ""
        try:
            pp = (top_items[0].get("poster_path") or "").strip()
            poster_url = f"{TMDB_IMG}{pp}" if pp else ""
        except Exception:
            poster_url = ""

        place_emoji = ["ü•á", "ü•à", "ü•â"]
        out = []
        out.append("üé¨ –Ø —Å–æ–±—Ä–∞–ª –¢–û–ü-3 —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (—Ä–µ–π—Ç–∏–Ω–≥):")
        out.append("")

        for i, it in enumerate(top_items):
            title = _item_title(it)
            year = _item_year(it)
            mt = (it.get("media_type") or "").strip() or ("tv" if it.get("name") else "movie")

            mt_ru = "—Å–µ—Ä–∏–∞–ª" if mt == "tv" else "—Ñ–∏–ª—å–º"

            ov = _short_overview(it, 130)
            url = _tmdb_item_url(it)

            line = f"{place_emoji[i]} {title}"
            if year:
                line += f" ({year})"
            line += f" ‚Äî {mt_ru}"
            out.append(line)

            if ov:
                out.append(f"–ö–æ—Ä–æ—Ç–∫–æ: {ov}")

            # ü•á ‚Äî –æ–¥–∏–Ω –ø–æ—Å—Ç–µ—Ä —á–µ—Ä–µ–∑ üñº
            if i == 0 and poster_url:
                out.append(f"üñº {poster_url}")
            else:
                # ü•àü•â ‚Äî —Ç–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∞ (–±–µ–∑ üñº), —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å —Ñ–æ—Ç–∫–∞–º–∏
                if url:
                    out.append(f"TMDb: {url}")

            out.append("")

        out.append("–ö–Ω–æ–ø–∫–∏: ‚úÖ –≠—Ç–æ –æ–Ω–æ / üîÅ –î—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã / üß© –£—Ç–æ—á–Ω–∏—Ç—å")
        reply = "\n".join(out).strip()

        if img_key:
            _vision_cache_set(img_key, reply)
        return reply

    # –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî –ø–∞–¥–∞–µ–º –Ω–∏–∂–µ –Ω–∞ WEB fallback
    # If parallel TMDb gave no best ‚Äî do WEB pipeline fallback (extract real title -> TMDb)
    if not best:
        try:
            # 1) no-SerpAPI first
            cands, tag = await web_to_tmdb_candidates(cand_list[0], use_serpapi=False)
            _d("vision.web.cands", use_serpapi=False, tag=tag, n=len(cands or []), sample=(cands or [])[:5])
            for c in (cands or [])[:12]:
                if _is_bad_media_query(c):
                    continue
                c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
                if not _good_tmdb_cand(c):
                    continue
                items = await _tmdb_best_effort(c, limit=5)
                items = [i for i in (items or []) if not i.get("adult")]
                if items:
                    used_query = c
                    break
        except Exception:
            pass

        # 2) SerpAPI only if key exists and still nothing
        if not items and (os.getenv("SERPAPI_API_KEY") or os.getenv("SERPAPI_KEY")):
            try:
                cands, tag = await web_to_tmdb_candidates(cand_list[0], use_serpapi=True)
                _d("vision.web.cands_serp", use_serpapi=True, tag=tag, n=len(cands or []), sample=(cands or [])[:5])
                for c in (cands or [])[:12]:
                    if _is_bad_media_query(c):
                        continue
                    c = _tmdb_sanitize_query(_normalize_tmdb_query(c))
                    if not _good_tmdb_cand(c):
                        continue
                    items = await _tmdb_best_effort(c, limit=5)
                    items = [i for i in (items or []) if not i.get("adult")]
                    if items:
                        used_query = c
                        break
            except Exception:
                pass

    if items:
        if user is not None:
            setattr(user, "assistant_mode", "media")
            setattr(user, "assistant_mode_until", now + timedelta(minutes=10))
            if session:
                await session.commit()

        uid = _media_uid(user)
        if uid:
            _media_set(uid, used_query or (cand_list[0] if cand_list else ""), items)

        reply = _format_media_ranked(
            used_query or (cand_list[0] if cand_list else ""),
            items,
            year_hint=_parse_media_hints(used_query or (cand_list[0] if cand_list else "")).get("year"),
            lang=lang,
            source="tmdb",
        )

        if img_key:
            _vision_cache_set(img_key, reply)
        return reply

    # --- Failsafe: Vision must always return text ---
    final_text = (out_text or "").strip()
    if final_text:
        return final_text
    return MEDIA_NOT_FOUND_REPLY_RU
